# Конспект лекции: Канальное кодирование

## 1. Организационные вопросы

- Лекции раз в неделю, практика объединена с лекциями.
- ДЗ проверяется в начале каждого занятия.
- Курс заканчивается к концу апреля (8--10 занятий).

Требования для автомата:

1. Конспект -- в течение 2 недель после занятия (Markdown или PDF, ссылка на GitHub).
2. ДЗ -- в СДО за 1 день до следующего занятия.
3. Активное участие на занятиях.
4. Допзадания -- при пропусках/долгах.

---

## 2. Введение

Курс -- про передачу данных через каналы с шумом. Передача данных -- не только телекоммуникации, но и чтение с флешки, жесткого диска и т.д. Данные всегда кодируются, чтобы устранить искажения аналогового мира.

---

## 3. Упрощенная модель цифровой системы связи

    Источник -> Кодер -> [Канал + Шум] -> Декодер -> Получатель

- Источник данных -- генерирует информационные символы (биты 0 и 1).
- Кодер источника -- превращает информационные символы в кодовые символы, добавляя избыточность.
- Канал с шумом -- среда передачи, где возникают искажения.
- Декодер источника -- восстанавливает исходные символы из принятых (с возможными ошибками).
- Получатель -- получает декодированные символы (в идеале совпадают с исходными).

Работаем с дискретными последовательностями из GF(2) -- нули и единицы. Кодовых символов обычно больше, чем информационных -- для внесения избыточности.

---

## 4. Двоичный симметричный канал (ДСК)

На вход подаются биты. Модель:

- 0 передан -> принят как 0 с вероятностью (1-p), как 1 с вероятностью p.
- 1 передана -> принята как 1 с вероятностью (1-p), как 0 с вероятностью p.

p -- переходная вероятность (вероятность ошибки одного символа).

Пример: p = 10^(-3) -- из 1000 символов хотя бы в одном ошибемся.

---

## 5. Примеры кодирования

### Пример 1: тройное повторение -- код (3, 1)

| ИС | КС  |
|----|-----|
| 0  | 000 |
| 1  | 111 |

Декодирование: мажоритарное (какой символ встречается 2 из 3 раз).

Почему не двойное? При 2 повторениях ошибку можно обнаружить, но не исправить. При 3 -- можно исправить по большинству.

Вероятность ошибки (2+ ошибки в блоке из 3):

    P_ош = 3 * p^2 * (1-p) + p^3 ~ 3 * 10^(-6)

Исправляет 1 ошибку на длине 3. Скорость R = 1/3.

### Пример 2: код (5, 2)

| ИС | КС    |
|----|-------|
| 00 | 00000 |
| 01 | 10110 |
| 10 | 01011 |
| 11 | 11101 |

Тоже исправляет 1 ошибку. Скорость R = 2/5.

### Сравнение

| Код          | k | n | R   | Исправляет |
|--------------|---|---|-----|------------|
| Без кода     | 1 | 1 | 1   | 0 ошибок   |
| Повторение   | 1 | 3 | 1/3 | 1 ошибку   |
| Код (5, 2)   | 2 | 5 | 2/5 | 1 ошибку   |

Скорость R = k/n -- отношение информационных символов к кодовым.

Код (5, 2) экономичнее: 2/5 > 1/3 при той же корректирующей способности.

Принцип декодирования: принимается кодовое слово, наименее отличающееся от принятого.

---

## 6. Теорема Шеннона

Шеннон (1948) -- основатель теории информации. Два направления:

- Кодирование источника -- убирает избыточность (пример: алгоритм Хаффмана).
- Канальное кодирование -- добавляет избыточность для исправления ошибок.

### Пропускная способность ДСК

    C = 1 - H(p)

    H(p) = -p * log2(p) - (1-p) * log2(1-p)    -- энтропия двоичного ансамбля

### Формулировка

При скорости R < C можно обеспечить сколь угодно малую вероятность ошибки -- за счет увеличения длины кода (и сложности кодирования/декодирования).

При R > C надежная передача невозможна.

### Числовой пример

    p = 10^(-3)
    C = 1 - H(10^(-3)) ~ 0.9888

Достаточно всего ~2% избыточности для сколь угодно высокой надежности.

Но: теорема доказана неконструктивно -- гарантирует существование хороших кодов, но не говорит, как их построить. До сих пор не найдены коды, достигающие границы Шеннона.

---

## 7. Вес и расстояние Хэмминга

### Вес Хэмминга

w(x) -- число ненулевых элементов вектора x. В двоичном случае -- число единиц.

Пример: w(01011) = 3.

### Расстояние Хэмминга

d(x, y) -- число позиций, в которых x и y различаются.

### Связь (двоичный случай)

    d(x, y) = w(x XOR y)
    d(x, 0) = w(x)

---

## 8. Минимальное расстояние кода

d_min = минимальное расстояние Хэмминга по всем парам различных кодовых слов.

### Пример: таблица расстояний для кода (5, 2)

|       | 00000 | 10110 | 01011 | 11101 |
|-------|-------|-------|-------|-------|
| 00000 | 0     | 3     | 3     | 4     |
| 10110 | 3     | 0     | 4     | 3     |
| 01011 | 3     | 4     | 0     | 3     |
| 11101 | 4     | 3     | 3     | 0     |

d_min = 3.

### Корректирующая способность

    t <= floor((d_min - 1) / 2)

Для d_min = 3: t <= floor(2/2) = 1 -- код исправляет 1 ошибку.

---

## 9. Линейные коды

### Определение

Линейный код -- сумма любых двух кодовых слов тоже кодовое слово:

    для любых x, y из C: x XOR y тоже в C

### Свойства

Для линейного двоичного кода:

- d(x, y) = w(x XOR y), а x XOR y -- тоже кодовое слово.
- Значит расстояние между любой парой = вес какого-то кодового слова.
- Поэтому d_min = минимальный вес ненулевого кодового слова.

Это упрощает вычисление: не нужно перебирать все пары.

### Общее определение

Линейный q-ичный (n, k)-код C -- любое k-мерное подпространство пространства всех векторов длины n над полем GF(q).

- q -- размер алфавита.
- k -- число информационных символов.
- n -- длина кодового слова.
- Число информационных слов: q^k.
- Скорость: R = k/n.

---

## 10. Порождающая матрица G

Линейный код -- подпространство, значит есть базис из k векторов. Все кодовые слова -- линейные комбинации базисных.

Порождающая матрица G -- матрица размера k x n, строки -- базисные векторы.

Кодирование: c = m * G, где m -- информационное слово.

### Пример: код (5, 2)

Базисные векторы: КС при ИС = 10 и ИС = 01.

|   | 1 | 2 | 3 | 4 | 5 |
|---|---|---|---|---|---|
| g1| 0 | 1 | 0 | 1 | 1 |
| g2| 1 | 0 | 1 | 1 | 0 |

Все кодовые слова через комбинации:

| m     | c = m * G | КС    |
|-------|-----------|-------|
| (0,0) | 0*g1 + 0*g2 | 00000 |
| (1,0) | 1*g1 + 0*g2 | 01011 |
| (0,1) | 0*g1 + 1*g2 | 10110 |
| (1,1) | 1*g1 + 1*g2 | 11101 |

---

## 11. Проверочная матрица H

Вектор h ортогонален коду, если для любого кодового слова c:

    c1*h1 + c2*h2 + ... + cn*hn = 0   (в GF(2))

Такой вектор -- проверка. Таких линейно независимых проверок (n - k) штук.

Проверочная матрица H -- матрица размера (n-k) x n из этих проверок.

Основное свойство: G * H^T = 0.

### Пример: код (5, 2), H размера 3x5

|   | 1 | 2 | 3 | 4 | 5 |
|---|---|---|---|---|---|
| h1| 1 | 0 | 1 | 0 | 0 |
| h2| 1 | 1 | 0 | 1 | 0 |
| h3| 0 | 1 | 0 | 0 | 1 |

Проверка: c * H^T = 0 для всех кодовых слов -- ok.

Порождающая и проверочная матрицы работают только для линейных кодов.